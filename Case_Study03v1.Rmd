---
title: "Comparação de desempenho de duas configurações de um algoritmo de otimização, parte II"
author:
- Alan Souza (Monitor)
- Alex Assis (Coordenador)
- Luíza Guimarães (Relator)  
- Patrícia Lucas (Verificadora)
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    fig_caption: yes
    number_sections: yes
  html_document:
    df_print: paged
    toc: true
bibliography: referencias.bib
link-citations: yes
geometry: margin=1in
subtitle: Estudo de Caso 03
---

\pagenumbering{gobble}
\begin{center}
$\vspace*{\fill}$

Programa de Pós-Graduação em Engenharia Elétrica - Universidade Federal de Minas Gerais
\end{center}
\newpage
\pagenumbering{arabic}
\tableofcontents
\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
```



```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Bibliotecas

library(ExpDE)
library(lessR) 
library(car) # teste de Durbin-Watson
library(nortest) # teste de normalidade e gráfico de probabilidade normal
library("effsize") # teste d-Cohen
library(stats)
```

```{r, include=FALSE,  echo=FALSE}
# Parâmetros fixos

selpars  <- list(name = "selection_standard")
stopcrit <- list(names = "stop_maxeval", maxevals = 50000, maxiter = 1000)
probpars <- list(name = "sphere", xmin = -seq(1,20), xmax = 20 +5 * seq(5,24))
delta <- 0.50 # Mínima diferença de importância prática (padronizada): ($d^* = \delta^*/\sigma$) = 0.25
alpha <- 0.05 # Significância desejada: $\alpha = 0.05$
potencia <- 0.80 # Potência mínima desejada (para o caso $d = d^*$): $\pi = 1 - \beta = 0.85$
nivelConfianca <- 1-alpha

# Equipe I

## Config 1
recpars1 <- list(name = "recombination_blxAlphaBeta", alpha = 0.4, beta = 0.4) 
mutpars1 <- list(name = "mutation_rand", f = 4)

## Config 2
recpars2 <- list(name = "recombination_wright")
mutpars2 <- list(name = "mutation_best", f = 4.8)
```


```{r, include=FALSE, echo=FALSE}
#setwd("C:/Users/Alan/Desktop/Mestrado UFMG/Disciplinas 2º semestre/Design and Analysis of Experiments/Group Projects/Estudo de caso III/Case Study 03")
```

```{r , include=FALSE, echo=FALSE}
## Leitura dos dados
dados <- read.csv("amostras.csv", header = FALSE, sep = ";")
colnames(dados) <- c("amostras1", "amostras2")
```

# Introdução

A análise do desempenho de algoritmos é uma tarefa recorrente de pesquisadores, onde buscam-se evidências concretas para a adoção de uma nova proposta em comparação com o estado da arte ou diferentes configurações do novo método proposto. Dessa forma, este trabalho apresenta a análise estatística de comparação pareada do desempenho de duas diferentes configurações de um método de otimização baseado no algorítmo de Evolução diferencial @storn1997differential. Para tanto foram geradas duas configurações do algorítmo através do pacote ExpDE @Campelo:2016:EIR:2908812.2908852 e aplicado no conjunto de funções de @rosenbrock1960automatic, que é um conjunto de funções não-convexas comumente utilizado como teste de desempenho de algoritmos de otimização. 

Os seguintes parâmetros experimentais são dados para este estudo:

- Mínima diferença de importância prática (padronizada): ($d^* = \delta^*/\sigma = 0.50$)
- Significância desejada: $\alpha = 0.05$
- Potência máxima desejada (para o caso $d = d^*$): $\pi = 1 - \beta = 0.80$


# Formulação das hipóteses de teste

Queremos saber se há alguma diferença no desempenho médio do algoritmo quando equipado com estas diferentes configurações, para o problema de teste utilizado. Por se tratar de uma análise pareada, o parâmetro de interesse é baseado na diferença do desempenho médio de cada configuração: $\mu_D = \mu_1 - \mu_2$ 

$$\begin{cases} H_0: \mu_D = 0&\\H_1: \mu_D \neq 0\end{cases}$$


# Cálculo do tamanho amostral

Foi utilizado o _power.t.test_ para calcular a quantidade de amostras necessárias para se obter um poder de teste de $80\%$.

```{r}
power <- power.t.test(delta = 0.5, sig.level = 0.05, power = 0.80,
                      type = "paired", alternative = "two.sided")

```
```{r eval=TRUE, echo=FALSE}
print(power)
```


A partir da realização do teste de potência, concluiu-se que o número necessário de amostras para atingir a potência de `r power$power` para o mínimo de relevância prática de `r power$delta`, é de `r ceiling(power$n)` amostras aplicadas para cada grupo.

# Coleta e tabulação dos dados
Os dados de interesse deste trabalho foram gerandos pelas funções de _Rosenbrock_ para dimensões de dim={3, 10, 17, 26, 29, 30, 31, 35, 41, 42, 48, 49, 51, 54, 57, 59, 68, 72, 84, 86, 90, 95, 96, 97, 103, 105, 107, 120, 122, 125, 131, 132, 134, 137}, escolhidas aleatoriamente no intervalo de 2 a 150. 
Para cada instância gerada nas configurações 1 e 2, foi feita a média de 30 repetições para cada instância, gerando `r length(dados$amostras1)` amostras .


* Amostra 1: 
```{r eval=TRUE, echo=FALSE}
head(dados$amostras1) 
```
* Amostra 2:
```{r eval=TRUE, echo=FALSE}
head(dados$amostras2) 
```

# Teste das hipóteses
Foi utilizado o teste _t-Student_ para diferença de médias de duas distribuições normais e variâncias desconhecidas, com nível de confiança de $0,95$.


```{r}
t_test <- t.test(dados$amostras1, dados$amostras2, 
                 paired = TRUE, 
                 alternative = "two.sided", 
                 conf.level = 0.95)

```
```{r eval=TRUE, echo=FALSE}
print(t_test)
```


Uma vez que o valor de p-valor = `r round(t_test$p.value, digits = 3)`  excede o valor de $\alpha = 0.05$, não temos evidências suficientes para rejeitar a hipótese nula no  nível de significância de 0.05 dada a hipótese nula de que o desempenho da configuração 2 resulta em um desempenho médio que difere da configuração 1. 



Teste-t e IC (intervalo de confiança) para duas Amostras, Configuração 1 (Conf. 1) e Configuração 2 (Config. 2):

Conf. 1: n = `r length(dados$amostras1)`, 
$\overline{x} =$ `r paste0(round(mean(dados$amostras1), digits=2))`, 
s = `r paste0(round(sd(dados$amostras1), digits=2))`

Conf. 2: n = `r length(dados$amostras2)`, 
$\overline{x} =$ `r paste0(round(mean(dados$amostras2), digits=2))`, 
s = `r paste0(round(sd(dados$amostras2), digits=2))`

Teste-t pareado:

t = `r t_test$statistic`

Graus de liberdade: `r t_test$parameter`

_p-valor_ = `r round(t_test$p.value, digits = 3)`



Intervalo de confiança de 95\%: `r paste0(round(t_test$conf.int[1]), digits=2)` a `r paste0(round(t_test$conf.int[2]), digits=2)`


# Estimação do tamanho de efeito e intervalo de confiança

O cálculo do tamanho de efeito com o método de *d de Cohen* é indicado quando as duas populações que estão sendo comparadas são contínuas e de distribuição normal. Podemos entender que quanto maior o tamanho do efeito, maior é o impacto que a variável central do experimento está causando e mais importante se torna o fato dela ter uma contribuição para a questão analisada, @LINDENAU:2012.


```{r}
s_agregado <- sqrt((var(dados$amostras1) + var(dados$amostras2))/2)

media = matrix(t_test$estimate)

d <- (media)/s_agregado

print(paste0("d = ", as.numeric(d)))
```
```{r echo=FALSE}
d_cohen <- cohen.d.default(dados$amostras1, dados$amostras2)
d_cohen
```


Com a estimação da magnitude de efeito _d_ = `r round(d, 4)`, temos que a probabilidade do intervalor de confiança (`r d_cohen$conf.int[1]`, `r d_cohen$conf.int[2]`) conter o verdadeiro valor de _d_ é de `r 100*d_cohen$conf.level`\%.
Dado o valor da estimativa do tamanho de efeito obtido acima, podemos observar que a distância entre as médias é pequena em termos do desvio padrão das amostras.


# Verificação das premissas dos testes

Como premissas dos testes realizados neste trabalho, foi assumido que as amostras possuem distribuição normal e com variâncias equivalentes.

## Premissa de normalidade

Para a premissa de normalidade, foi realizado o teste _Shapiro-Wilk_, que assume como hipótese nula que a distribuição dos dados provém de uma distribuição normal. Como o _p-valor_ foi maior que $\alpha = 0.01$, não temos evidências suficientes para rejeitar a hipótese nula de normalidade dos dados. 

```{r}
difTimes<-with(dados,amostras1-amostras2)

shapiro.test(difTimes)
```

Foi realizado ainda uma análise gráfica da normalidade que corrobora com as conclusões obitidas através do teste anterior, uma vez que todos os pontos estão próximos da reta, descrevendo a distribuição normal dos dados.

```{r}
library(car)
qqPlot(difTimes,
       pch=16,
       cex=1.5,
       las=1)

```



## Premissa de homogeneidade de variâncias

Para a premissa de homogeneidade de variâncias foi realizado o teste Fligner-Killeen, que não apresentou evidências suficientes pra rejeitar a hipótese nula de igualdade da variância dos dados com nivel de confiança de 99\%:


```{r}
dados_amostrais <- list(dados$amostras1, dados$amostras2)
fligner.test(dados_amostrais)

```

```{r}
media1 <- mean(dados$amostras1)
media2 <- mean(dados$amostras2)
m1 <- rep(media1, nrow(dados))
m2 <- rep(media2, nrow(dados))
m <- rbind(m1,m2)
resid1 <- dados$amostras1 - media1
resid2 <- dados$amostras2 - media2
dados_plot <- rbind(dados$amostras1, dados$amostras2)
residuo <- rbind(resid1, resid2)

plot(m, residuo)
```
Fizemos ainda uma análise gráfica das duas distribuições. Podemos notar que a variação dos dados é bem semelhante em ambos os casos.
```{r}
par(mfrow = c(1,2))
boxplot(dados$amostras1)
boxplot(dados$amostras2)
```



## Premissa de independência

Consideramos a dependência dos dados dada a maneira o experimento foi executado, uma vez que os dados foram gerados do mesmo computador com as mesmas configurações de _hardware_.


# Conclusões

De acordo com os resultados obtidos pelo _t.test_, observamos que não existem evidências suficientes para rejeitar a hipótese nula de igualdade do desempenho médio dos dois algoritmos, com um nível de confiança 95\%. O tamanho de efeito calculado a partir do estimador *d de Cohen* corrobora com o resultado do teste t, portanto, não há diferenças significativas entre as médias, isso não indica propriamente que as mesmas sejam iguais, mas assegura que os resultados apresentam efeitos relevantes na prática.

Portanto, conclui-se que não há evidências suficiente entre as diferença no desempenho médio do algoritmo quando equipado com essas configurações.

Como os dois algoritmos apresentam performaces equivalentes, é necessário avaliar outros critérios para decidir qual a opção do melhor algoritmo, como por exemplo a interface amigavel, entre outros.


# Discussão sobre possíveis limitações do estudo e sugestões de melhoria.
Uma limitação do estudo é o tempo necessário para geração de um grande número de amostras.

# Referências