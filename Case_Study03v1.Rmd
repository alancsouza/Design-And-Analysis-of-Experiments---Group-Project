---
title: "Comparação de configurações de um método de otimização"
author:
- Alan Souza (Monitor)
- Alex Assis (Coordenador)
- Luíza Guimarães (Relator)  
- Patrícia Lucas (Verificadora)
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    fig_caption: yes
    number_sections: yes
  html_document:
    df_print: paged
    toc: true
bibliography: referencias.bib
link-citations: yes
geometry: margin=1in
subtitle: Estudo de Caso 02
---

\pagenumbering{gobble}
\begin{center}
$\vspace*{\fill}$

Programa de Pós-Graduação em Engenharia Elétrica - Universidade Federal de Minas Gerais
\end{center}
\newpage
\pagenumbering{arabic}
\tableofcontents
\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r, echo=FALSE, message=FALSE, warning=FALSE}
#
# Bibliotecas
#
library(ExpDE)
#install.packages("lessR")
#library(lessR) 
#install.packages("car")
library(car) # teste de durbin Watson
#install.packages("nortest")
library(nortest) # teste de normalidade e grÃ¡fico de probabilidade normal
#install.packages("effsize") 
library("effsize") # teste d-Cohen
library(stats)
```

```{r, include=FALSE,  echo=FALSE}
#
# Parâmetros fixos
#
selpars  <- list(name = "selection_standard")
stopcrit <- list(names = "stop_maxeval", maxevals = 50000, maxiter = 1000)
probpars <- list(name = "sphere", xmin = -seq(1,20), xmax = 20 +5 * seq(5,24))
delta <- 0.50 # Mínima diferença de importância prática (padronizada): ($d^* = \delta^*/\sigma$) = 0.25
alpha <- 0.05 # Significância desejada: $\alpha = 0.05$
potencia <- 0.80 # Potência mínima desejada (para o caso $d = d^*$): $\pi = 1 - \beta = 0.85$
nivelConfianca <- 1-alpha

# Equipe I

## Config 1
recpars1 <- list(name = "recombination_blxAlphaBeta", alpha = 0.4, beta = 0.4) 
mutpars1 <- list(name = "mutation_rand", f = 4)
popsize1 <- 230

## Config 2
recpars2 <- list(name = "recombination_wright")
mutpars2 <- list(name = "mutation_best", f = 4.8)
popsize2 <- 113
```


```{r, include=FALSE, echo=FALSE}
setwd("C:/Users/Alan/Desktop/Mestrado UFMG/Disciplinas 2º semestre/Design and Analysis of Experiments/Group Projects/Estudo de caso III")
```

```{r , include=FALSE, echo=FALSE}
## Leitura dos dados
dados <- read.csv("amostras.csv")
colnames(dados) <- c("amostras1", "amostras2")
head(dados)

dim(dados)
```


# Formulação das hipóteses de teste
**Parâmetro de interesse:** Os parâmetros de interesse são $\mu_1$ e $\mu_2$, o desempenho médio do algoritmo usando as configurações 1 e 2. 

Queremos saber se há alguma diferença no desempenho médio do algoritmo quando equipado com estas diferentes configurações, para o problema de teste utilizado. $\mu_1-\mu_2 = 0$

$$\begin{cases} H_0: \mu_D = 0&\\H_1: \mu_D > 0\end{cases}$$


**Hipótese nula:** $H_0: \mu_1-\mu_2 = 0 \quad \Rightarrow \quad\mu_1 = \mu_2$

A hipótese nula considera que o desempenho médio dos algoritmos não apresenta diferenças significativas.

**Hipótese alternativa:** $H_1: \mu_1 >= \mu_2$

Na hipótese alternativa considera que existe diferença no desempenho médio das configurações 1 e 2.

# Cálculo do tamanho amostral

Foi utilizado o _power.t.test_ para calcular o tamanho amostral necessário para se obter um poder de teste de $85\%$.

```{r}
power <- power.t.test(delta = 0.5, sig.level = 0.05, power = 0.80,
                      type = "paired", alternative = "one.sided")
print(power)
```
Os seguintes parâmetros experimentais são dados para este estudo:

- Mínima diferença de importância prática (padronizada): ($d^* = \delta^*/\sigma = 0.50$)
- Significância desejada: $\alpha = 0.05$
- Potência máxima desejada (para o caso $d = d^*$): $\pi = 1 - \beta = 0.80$

A partir da realização do teste t, o número necessário de amostras para atingir a potência de `r power$power` para o mínimo de relevância prática de `r power$delta`, é de `r ceiling(power$n)` amostras aplicadas para cada grupo.

# Coleta e tabulação dos dados
Os dados de interesse deste trabalho foram gerandos pelas funções de _Rosenbrock_ para dimensões de dim={3, 10, 17, 26, 29, 30, 31, 35, 41, 42, 48, 49, 51, 54, 57, 59, 68, 72, 84, 86, 90, 95, 96, 97, 103, 105, 107}, escolhidas aleatoriamente no intervalo de 2 à 150. 
Para cada instância gerada nas configurações 1 e 2, foi feita a média de 30 repetições para cada instância.

Para cada configuração foram coletadas `r length(dados$amostras1)` valores das melhores execuções para análise. Dados:

* Amostra 1: 
```{r eval=TRUE, echo=FALSE}
head(dados$amostras1) 
```
* Amostra 2:
```{r eval=TRUE, echo=FALSE}
head(dados$amostras2) 
```

# Teste das hipóteses
Foi utilizado o teste _t-Student_ para diferença de médias de duas distribuições normais e variâncias desconhecidas, com nível de confiança de $0,95$.


```{r}
t_test <- t.test(dados$amostras1, dados$amostras2, 
                 paired = TRUE, 
                 alternative = "greater", 
                 conf.level = 0.95)
print(t_test)
```
Uma vez que o valor de $p-valor = $ `r t_test$p.value`  excede o valor de $\alpha = 0.05$, não temos evidências suficientes para rejeitar a hipótese nula no  nível de significância de 0.05 dada a hipótese nula de que o desempenho da configuração 2 resulta em um desempenho médio que difere da configuração 1. 

Uma vez que o valor de $p-valor = $ `r t_test$p.value`  excede o valor de $\alpha = 0.05$, não temos evidências suficientes para rejeitar a hipótese nula no  nível de significância de 0.05 não temos evidência fortes para concluir que o desempenho da configuração 2 resulta em um desempenho médio que difere da configuração 1. 


Teste-t e IC (intervalo de confiança) para duas Amostras, Configuração 1 (Conf. 1) e Configuração 2 (Config. 2):

Conf. 1: n = `r length(dados$amostras1)`, $\bar x =$ `r mean(dados$amostras1)`, s =  `r sd(dados$amostras1)`

Conf. 2: n = `r length(dados$amostras2)`, $\bar x =$ `r mean(dados$amostras2)`, s =  `r sd(dados$amostras2)`

Diferença de $\mu_1 - \mu_2$

Teste-t da diferença:

t = `r t_test$statistic`

Graus de liberdade: `r t_test$parameter`

_p-valor_ = `r t_test$p.value`

Intervalo de confiança de $95\%$ para a diferença: ($-92340.71$ a $+\infty$)


# Estimação do tamanho de efeito e intervalo de confiança

O cálculo do tamanho de efeito com o método de *d de Cohen* é indicado quando as duas populações que estão sendo comparadas são contínuas e de distribuição normal. Podemos entender que quanto maior o tamanho do efeito, maior é o impacto que a variável central do experimento está causando e mais importante se torna o fato dela ter uma contribuição para a questão analisada. @LINDENAU:2012


```{r}
s_agregado <- sqrt((var(dados$amostras1) + var(dados$amostras2))/2)

media = matrix(t_test$estimate)

d <- (media)/s_agregado

print(paste0("d = ", as.numeric(d)))

d_cohen <- cohen.d.default(dados$amostras1, dados$amostras2)

```
Para o nosso problema assuma a igualdade da variância do desempenho entre as populações. Com a magnitude de efeito _d_ = `r d` de *Cohen*, assim de acordo com a tabela $z$ padronizada, em casos futuros cerca de $59\%$ das amostras do grupo experimental excederão o valor médio do grupo.

O Intervalo de confiança (IC) de `r 100*d_cohen$conf.level` para a diferença: IC = (`r d_cohen$conf.int[1]`, `r d_cohen$conf.int[2]`),
temos que o tamanho de efeito $d \in IC$, desta forma temos que _d_ está dentro do intervalo de valores plausíveis.

Dado o valor de *d* obtido acima, podemos observar que a distância entre as médias é bem pequena.


# Verificação das premissas dos testes

Como premissas do teste, foi assumido que as amostras possuem distribuição normal, com variâncias equivalentes e que são independentes.


## Premissa de normalidade

Para a premissa de normalidade, foi realizado o teste _Shapiro-Wilk_, que assume como hipótese nula que a distribuição dos dados provém de uma distribuição normal. O _p-valor_ de cada amostra apresentou valores bem baixos, rejeitando-se a hipótese nula de normalidade dos dados. 

```{r}
difTimes<-with(dados,
            amostras1-amostras2)
head(difTimes)
```

## teste gráfico

```{r}
library(car)
qqPlot(difTimes,
       pch=16,
       cex=1.5,
       las=1)

# Highlight the observed outlier
indx<-which(difTimes==max(difTimes))
pt<-qqnorm(difTimes,plot.it=F)
points(pt$x[indx],pt$y[indx],pch=1,cex=2,col=2)

```


```{r}
shapiro.test(difTimes)
```

Foi realizado ainda o teste Kolmogorov-Smirnov, o qual corrobora com os resultados apresentados no teste Shapiro-Wilk:
```{r}
ks.test(dados$amostras1, y =  "pnorm", alternative = "less")
ks.test(dados$amostras2, y =  "pnorm", alternative = "less")
```

Entretanto, segundo @Campelo2015-01, é importante comparar os resultados juntamente com uma análise gráfica. Dessa forma, a normalidade dos dados das amostras 1 e 2 foi analisada nos gráficos de probabilidade normal abaixo:

```{r}
#
# Gráfico de probabilidade normal
#
par(mfrow = c(1,2))
qqnorm(dados$amostras1, main="Amostra 1", xlab = "",
       ylab = "Desempenho", pch = 20)
qqline(dados$amostras1, lty = 2, col = "red")
qqnorm(dados$amostras2, main="Amostra 2", xlab = "",
       ylab = "Desempenho", pch = 20, col = "blue")
qqline(dados$amostras2, lty = 2, col = "red")

```

```{r}

par(mfrow = c(1,2))
qqPlot(dados$amostras1, pch=16, cex=1, las=1)
qqPlot(dados$amostras2, pch=16, cex=1, las=1)
```
Pode-se observar nos gráficos acima que a distribuição possui um calda acentuada à direita, caracterizando que não se trata de uma distribuição normal de dados.

Como a premissa de normalidade não trata necessariamente da distribuição normal dos dados, e sim da distribuição normal das médias dos dados, foram coletadas amostras dos dados de teste de forma aleatória, calculada a média e criado um vetor de médias composto pela média de cada sub-amostra da amostra da população.

```{r}
# Função que gera sub-amostras e calcula a média
calcula_medias_amostrais <- function(dados){
  medias_amostrais <- rep(0,20)
  
  for(idx in 1:length(medias_amostrais)){
    medias_amostrais[idx] <- mean(sample(dados, 50, replace = FALSE))
  }
  
  medias_amostrais
}

```

```{r}
#medias_amostras1 <- calcula_medias_amostrais(dados$amostras1)
#medias_amostras2 <- calcula_medias_amostrais(dados$amostras2)

#par(mfrow = c(1,2))
#hist(medias_amostras1)
#hist(medias_amostras2)

```

```{r}
##shapiro.test(medias_amostras2)
```
Embora o _p-valor_ do teste Shapiro-Wilk para as médias indica evidências para aceitar a hipótese de normalidade das médias, resta verificar a validade desse subamostragem para assumir de fato a premissa de normalidade, dado que o histograma das médias não se comporta exatamente como uma gaussiana.

Para casos como este, é recomendada a realização das versões dos testes não-paramétricos (teste t e de potência), entretanto, dada a robustez do teste t para amostras que não são distribuídas por uma normal, optou-se por manter os testes paramétricos.


## Premissa de homogeneidade de variâncias

Para a premissa de homogeneidade de variâncias foi realizado o teste Fligner-Killeen, que apresentou evidências pra se aceitar a hipótese nula de igualdade da variância dos dados:


```{r}
dados_amostrais <- list(dados$amostras1, dados$amostras2)
fligner.test(dados_amostrais)

```

```{r}
media1 <- mean(dados$amostras1)
media2 <- mean(dados$amostras2)
m1 <- rep(media1, 27)
m2 <- rep(media2, 27)
m <- rbind(m1,m2)
resid1 <- dados$amostras1 - media1
resid2 <- dados$amostras2 - media2
dados_plot <- rbind(dados$amostras1, dados$amostras2)
residuo <- rbind(resid1, resid2)

plot(m, residuo)
```




## Premissa de independência

Uma vez que, segundo @Campelo2015-01, não existe um teste geral para a análise da premissa de independência, foi realizado o teste Durbin-Watson de autocorrelação serial.


```{r}
durbinWatsonTest(lm(dados$amostras1 ~ 1))
```

```{r}
durbinWatsonTest(lm(dados$amostras2 ~ 1))
```

Dessa forma, através da interpretação do _p-valor_, não podemos rejeitar a hipótese nula de independência dos dados em $99\%$.

# Conclusões

De acordo com os resultados obtidos pelo _t.test_, realizado após o cálculo do novo tamanho amostral, observamos que a hipótese nula não pode ser rejeitada. É necessário recalcular o tamanho amostral, para encontrar o número mínimo de amostras necessárias para evitar o erro tipo II, ou seja, a hipótese nula não poder ser rejeitada enquanto ela deveria ser rejeitada. Portanto, o desempenho médio dos algoritmos não apresenta diferenças significativas.

A pequena distância entre as médias, obtida através do tamanho de efeito calculado, $d = $ `r d`, não indica propriamente que as médias sejam iguais, ou que não exista um efeito substantivo, significa sim que não houve evidência suficientemente forte para provar que essas diferenças eram significativas.

Dessa forma, dada a similaridade entre as duas configurações, conclui-se que ambas podem ser utilizados uma vez que apresentam performances equivalentes.

# Discussão sobre possíveis limitações do estudo e sugestões de melhoria.
Uma limitação do estudo é o tempo necessário para geração de um grande número de amostras.

# Questão bônus

_Há alguma diferença na variância do desempenho das duas configurações para o problema de teste? Como esta eventual diferença (caso haja) impacta nas suas recomendações relativas à adoção de uma dada configuração?_


R: Para avaliar se há diferenÃ§a significante na variância do desempenho foi utilizado o método *Friedman two-way analysis of variance*, com n @Campelo:2016:EIR:2908812.2908852

**Hipótese nula:** $H_0: \mu_1 = \mu_2$

A hipótese nula considera que a variância média do desempenho das configurações 1 e 2 não apresenta diferenças significativas.

**Hipótese alternativa:** $H_1: \mu_1 \ne \mu_2$

Na hipótese alternativa considera-se que existe diferença na variância média do desempenho das configurações 1 e 2.

```{r, echo=FALSE}

# Teste de Friedman baseado no artigo: Experimental Investigation of Recombination Operators for Differential Evolution
#
m_mydata <- matrix(dados$amostras1, ncol = 1)
m_mydata <- cbind(m_mydata, dados$amostras2)
dimnames(m_mydata) <- list(c(1:nrow(m_mydata)), c("Amostra1", "Amostra2"))

friedman = friedman.test(m_mydata)
print(friedman)
```
Com  tableofcontent = 0.7422, não temos evidências suficientes para rejeitar a hipótese nula, portanto não podemos afirmar que existem diferenças significativas na variância das amostras analisadas.

A dispersão das amostras pode ser visualizada no gráfico abaixo:


```{r, echo=FALSE}
boxplot(m_mydata, ylab = "Desempenho")
```
# Referências